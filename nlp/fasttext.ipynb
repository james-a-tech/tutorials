{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeding using fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the quick brown fox jumps over the lazy dog',\n",
       " 'the fast brown cat jumps over the sleeping dog',\n",
       " 'a quick fox runs fast']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "        \"the quick brown fox jumps over the lazy dog\",\n",
    "        \"the fast brown cat jumps over the sleeping dog\",\n",
    "        \"a quick fox runs fast\",\n",
    "    ]\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the text and generate vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'brown',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'fast',\n",
       " 'fox',\n",
       " 'jumps',\n",
       " 'lazy',\n",
       " 'over',\n",
       " 'quick',\n",
       " 'runs',\n",
       " 'sleeping',\n",
       " 'the']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(sentences):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from the given sentences.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        vocab.update(words)\n",
    "    return sorted(vocab)\n",
    "\n",
    "vocab = build_vocabulary(sentences)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext generates n-grams which is used for capturing out of vocbulary words during prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Grams : []\n",
      "N-Grams : ['br', 'ro', 'ow', 'wn']\n",
      "N-Grams : ['ca', 'at']\n",
      "N-Grams : ['do', 'og']\n",
      "N-Grams : ['fa', 'as', 'st']\n",
      "N-Grams : ['fo', 'ox']\n",
      "N-Grams : ['ju', 'um', 'mp', 'ps']\n",
      "N-Grams : ['la', 'az', 'zy']\n",
      "N-Grams : ['ov', 've', 'er']\n",
      "N-Grams : ['qu', 'ui', 'ic', 'ck']\n",
      "N-Grams : ['ru', 'un', 'ns']\n",
      "N-Grams : ['sl', 'le', 'ee', 'ep', 'pi', 'in', 'ng']\n",
      "N-Grams : ['th', 'he']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "def generate_n_grams(vocab, n_gram_size):\n",
    "    n_grams_words = set()\n",
    "    for word in vocab:\n",
    "        temp = []\n",
    "        for size in range(2, n_gram_size + 1):\n",
    "            for i in range(len(word) - size + 1):  # Adjusted range to include valid n-grams\n",
    "                n_gram = word[i:i + size]\n",
    "                temp.append(n_gram)  # Append to the list of n-grams for the current word\n",
    "                n_grams_words.add(n_gram)  # Add to the set of all n-grams\n",
    "        print(f'N-Grams : {temp}')\n",
    "    return n_grams_words\n",
    "\n",
    "n_grams = generate_n_grams(vocab, 2)\n",
    "print(len(n_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 52\n"
     ]
    }
   ],
   "source": [
    "vocab.extend(n_grams)\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "for i , word in enumerate(vocab):\n",
    "    word_to_index[word] = i\n",
    "    index_to_word[i] = word\n",
    "\n",
    "print(f'Vocabulary size : {len(word_to_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and generate word embedding\n",
    "Skipgram approch is used for traning where the target add is used as input and context words as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'quick'),\n",
       " ('quick', 'the'),\n",
       " ('quick', 'brown'),\n",
       " ('brown', 'the'),\n",
       " ('brown', 'quick'),\n",
       " ('brown', 'fox'),\n",
       " ('fox', 'quick'),\n",
       " ('fox', 'brown'),\n",
       " ('fox', 'jumps'),\n",
       " ('jumps', 'brown'),\n",
       " ('jumps', 'fox'),\n",
       " ('jumps', 'over'),\n",
       " ('over', 'fox'),\n",
       " ('over', 'jumps'),\n",
       " ('over', 'the'),\n",
       " ('the', 'jumps'),\n",
       " ('the', 'over'),\n",
       " ('the', 'lazy'),\n",
       " ('lazy', 'over'),\n",
       " ('lazy', 'the'),\n",
       " ('lazy', 'dog'),\n",
       " ('dog', 'the'),\n",
       " ('dog', 'lazy'),\n",
       " ('the', 'fast'),\n",
       " ('fast', 'the'),\n",
       " ('fast', 'brown'),\n",
       " ('brown', 'the'),\n",
       " ('brown', 'fast'),\n",
       " ('brown', 'cat'),\n",
       " ('cat', 'fast'),\n",
       " ('cat', 'brown'),\n",
       " ('cat', 'jumps'),\n",
       " ('jumps', 'brown'),\n",
       " ('jumps', 'cat'),\n",
       " ('jumps', 'over'),\n",
       " ('over', 'cat'),\n",
       " ('over', 'jumps'),\n",
       " ('over', 'the'),\n",
       " ('the', 'jumps'),\n",
       " ('the', 'over'),\n",
       " ('the', 'sleeping'),\n",
       " ('sleeping', 'over'),\n",
       " ('sleeping', 'the'),\n",
       " ('sleeping', 'dog'),\n",
       " ('dog', 'the'),\n",
       " ('dog', 'sleeping'),\n",
       " ('a', 'quick'),\n",
       " ('quick', 'a'),\n",
       " ('quick', 'fox'),\n",
       " ('fox', 'a'),\n",
       " ('fox', 'quick'),\n",
       " ('fox', 'runs'),\n",
       " ('runs', 'quick'),\n",
       " ('runs', 'fox'),\n",
       " ('runs', 'fast'),\n",
       " ('fast', 'fox'),\n",
       " ('fast', 'runs')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        start = max(0,i -2)\n",
    "        end = min(len(words),i + 2)\n",
    "        for j in range( start,end):\n",
    "            if j == i:\n",
    "                continue\n",
    "            training_data.append((words[i],words[j]))\n",
    "            \n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and generating word embeddings without negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 1.8562\n",
      "Epoch 2000, Loss: 0.6182\n",
      "Epoch 3000, Loss: 0.3601\n",
      "Epoch 4000, Loss: 0.2507\n",
      "Epoch 5000, Loss: 0.1908\n",
      "Epoch 6000, Loss: 0.1533\n",
      "Epoch 7000, Loss: 0.1277\n",
      "Epoch 8000, Loss: 0.1091\n",
      "Epoch 9000, Loss: 0.0951\n",
      "Epoch 10000, Loss: 0.0841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 10000\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "# Weight initialization\n",
    "W1 = np.random.uniform(-1, 1, (vocab_size, embedding_dim))  # Input to hidden weights\n",
    "W2 = np.random.uniform(-1, 1, (embedding_dim, vocab_size))  # Hidden to output weights\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_vector(word, word_to_index):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target, context in training_data:\n",
    "        # Forward pass\n",
    "        context_vectors = np.sum([one_hot_vector(target, word_to_index)], axis=0)\n",
    "        h = np.dot(context_vectors, W1)  # Hidden layer\n",
    "        u = np.dot(h, W2)  # Output layer\n",
    "        y_pred = np.exp(u) / np.sum(np.exp(u)) # Softmax activation\n",
    "        \n",
    "        # Calculate loss (cross-entropy)\n",
    "        target_one_hot = one_hot_vector(target, word_to_index)\n",
    "        loss += -np.sum(target_one_hot * np.log(y_pred + 1e-8))\n",
    "\n",
    "        # Backpropagation\n",
    "        e = y_pred - target_one_hot\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(context_vectors, np.dot(W2, e))\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "N-Grams : []\n",
      "\n",
      "Words similar to 'fox':\n",
      "fox: 1.0000\n",
      "og: 0.5997\n",
      "un: 0.4414\n",
      "ng: 0.4356\n",
      "ps: 0.3887\n"
     ]
    }
   ],
   "source": [
    " def get_word_vector(word):\n",
    "        \"\"\"Get vector representation of a word including its n-grams\"\"\"\n",
    "        if word in word_to_index:\n",
    "            word_idx = word_to_index[word]\n",
    "            word_vec = W1[word_idx].copy()\n",
    "        else:\n",
    "            word_vec = np.zeros(embedding_dim)\n",
    "        \n",
    "        # Add n-gram vectors\n",
    "        ngrams = generate_n_grams(word,2)\n",
    "        for ngram in ngrams:\n",
    "            if ngram in self.word_to_index:\n",
    "                ngram_idx = word_to_index[ngram]\n",
    "                word_vec += W1[ngram_idx]\n",
    "        \n",
    "        return word_vec / (len(ngrams) + 1)  # Average the vectors\n",
    "\n",
    "def most_similar(word, top_n=5):\n",
    "        \"\"\"Find most similar words\"\"\"\n",
    "        word_vec = get_word_vector(word)\n",
    "        similarities = {}\n",
    "        \n",
    "        for other_word in word_to_index:\n",
    "            other_vec = get_word_vector(other_word)\n",
    "            similarity = np.dot(word_vec, other_vec) / (\n",
    "                np.linalg.norm(word_vec) * np.linalg.norm(other_vec)\n",
    "            )\n",
    "            similarities[other_word] = similarity\n",
    "        \n",
    "        return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "similar_words = most_similar(\"fox\")\n",
    "print(\"\\nWords similar to 'fox':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
