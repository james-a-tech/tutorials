{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie', 'was', 'boring'], ['movie', 'actions', 'were', 'very', 'good'], ['movie', 'was', 'good'], ['movie', 'story', 'was', 'very', 'bad']]\n",
      "Vocab size: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the training data\n",
    "# corpus = [\n",
    "#     \"the quick brown fox jumped over the lazy dog\",\n",
    "#     \"the dog barked at the fox\",\n",
    "#     \"the lazy dog slept\",\n",
    "#     \"the quick fox was very quick\"\n",
    "# ]\n",
    "corpus = [\n",
    "    \"Movie was boring\",\n",
    "    \"Movie actions were very good\",\n",
    "    \"Movie was good\",\n",
    "    \"Movie story was very bad\"\n",
    "]\n",
    "\n",
    "# Preprocessing: tokenize the sentences and build the vocabulary\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [sentence.lower().split() for sentence in corpus]\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary and mappings\n",
    "def build_vocab(tokenized_corpus):\n",
    "    vocab = set()\n",
    "    for sentence in tokenized_corpus:\n",
    "        vocab.update(sentence)\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "word_to_index, index_to_word = build_vocab(tokenized_corpus)\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print(tokenized_corpus)\n",
    "print(f'Vocab size: {vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['was'], 'movie'),\n",
       " (['movie', 'boring'], 'was'),\n",
       " (['was'], 'boring'),\n",
       " (['actions'], 'movie'),\n",
       " (['movie', 'were'], 'actions'),\n",
       " (['actions', 'very'], 'were'),\n",
       " (['were', 'good'], 'very'),\n",
       " (['very'], 'good'),\n",
       " (['was'], 'movie'),\n",
       " (['movie', 'good'], 'was'),\n",
       " (['was'], 'good'),\n",
       " (['story'], 'movie'),\n",
       " (['movie', 'was'], 'story'),\n",
       " (['story', 'very'], 'was'),\n",
       " (['was', 'bad'], 'very'),\n",
       " (['very'], 'bad')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define context and target creation\n",
    "def generate_training_data(tokenized_corpus, window_size=1):\n",
    "    training_data = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            context = []\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0 and 0 <= i + j < len(sentence):\n",
    "                    context.append(sentence[i + j])\n",
    "            target = word\n",
    "            training_data.append((context, target))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(tokenized_corpus)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 6.0661\n",
      "Epoch 2000, Loss: 5.9807\n",
      "Epoch 3000, Loss: 5.9451\n",
      "Epoch 4000, Loss: 5.9201\n",
      "Epoch 5000, Loss: 5.9005\n",
      "Epoch 6000, Loss: 5.8848\n",
      "Epoch 7000, Loss: 5.8720\n",
      "Epoch 8000, Loss: 5.8616\n",
      "Epoch 9000, Loss: 5.8530\n",
      "Epoch 10000, Loss: 5.8460\n",
      "Word: story, Embedding: [ 0.61296695 -2.02968151  0.40999344 -1.80774536  0.71139503 -2.15146414\n",
      " -1.49340307 -0.17002308  0.84067125  1.03293581]\n",
      "Word: bad, Embedding: [ 0.81443191 -0.16625505 -1.16117618  1.10794049  0.73504063  1.21059665\n",
      " -0.473284   -0.93869332  2.10504319  1.59647903]\n",
      "Word: were, Embedding: [ 1.13467847  1.06144307  0.60877506 -0.13299855 -1.95872094  0.88250022\n",
      "  0.05692184  0.52090016 -0.15463408  1.34342115]\n",
      "Word: actions, Embedding: [ 2.11715297  0.31220676  1.67147372 -2.45587517  0.71064448 -0.94340327\n",
      " -0.89678103  0.21794937 -0.9732588   1.51023869]\n",
      "Word: was, Embedding: [ 0.63498284 -0.64684698 -0.94248559 -0.95003585 -0.57050697 -0.05634887\n",
      "  0.25353939 -0.53652473 -1.17099228 -0.6489178 ]\n",
      "Word: very, Embedding: [-0.42425718  1.31711496 -1.42379643  2.03825336 -0.35816106  0.63587745\n",
      " -0.53646782  0.88459197 -0.8380074  -1.29198718]\n",
      "Word: good, Embedding: [ 1.19987539 -0.93056854 -0.3513764   1.68550414 -0.74667907 -0.9193601\n",
      " -0.14941092  0.05101492  0.86746688  0.64050134]\n",
      "Word: boring, Embedding: [ 0.52110321 -1.25510648 -1.70154096  0.54034309 -0.2521245  -0.83102616\n",
      " -0.36646365 -0.43119384  1.36078658 -0.57437535]\n",
      "Word: movie, Embedding: [-2.70402077  0.296106    1.25047463  0.96051636  0.64215549  0.051694\n",
      " -1.66700278 -1.32042077  0.03794682  0.61195553]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "# Weight initialization\n",
    "W1 = np.random.uniform(-1, 1, (vocab_size, embedding_dim))  # Input to hidden weights\n",
    "W2 = np.random.uniform(-1, 1, (embedding_dim, vocab_size))  # Hidden to output weights\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_vector(word, word_to_index):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for context, target in training_data:\n",
    "        # Forward pass\n",
    "        context_vectors = np.sum([one_hot_vector(word, word_to_index) for word in context], axis=0)\n",
    "        h = np.dot(context_vectors, W1)  # Hidden layer\n",
    "        u = np.dot(h, W2)  # Output layer\n",
    "        y_pred = np.exp(u) / np.sum(np.exp(u)) # Softmax activation\n",
    "        \n",
    "        # Calculate loss (cross-entropy)\n",
    "        target_one_hot = one_hot_vector(target, word_to_index)\n",
    "        loss += -np.sum(target_one_hot * np.log(y_pred + 1e-8))\n",
    "\n",
    "        # Backpropagation\n",
    "        e = y_pred - target_one_hot\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(context_vectors, np.dot(W2, e))\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Display word embeddings\n",
    "for word, idx in word_to_index.items():\n",
    "    print(f'Word: {word}, Embedding: {W1[idx]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Euclidian distances b/w the word embeding to see the similarity b/w them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', -0.0),\n",
       " ('boring', -2.4396124005917725),\n",
       " ('bad', -3.461821912031705),\n",
       " ('were', -3.8365481664176646),\n",
       " ('was', -3.8466084848655866),\n",
       " ('very', -4.361414069388088),\n",
       " ('story', -4.4708995591898075),\n",
       " ('movie', -5.2517917415193605),\n",
       " ('actions', -5.521263175819463)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def similar(target='beautiful'):\n",
    "    target_index = word_to_index[target] \n",
    "    scores = Counter() \n",
    "    for word,index in word_to_index.items(): \n",
    "        raw_difference = W1[index] - (W1[target_index]) \n",
    "        squared_difference = raw_difference * raw_difference \n",
    "        scores[word] = -math.sqrt(sum(squared_difference)) \n",
    "\n",
    "    return scores.most_common(10)\n",
    "\n",
    "similar('good')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
